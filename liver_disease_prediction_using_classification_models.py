# -*- coding: utf-8 -*-
"""Liver_Disease_Prediction_using_Classification_Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JQOkMWECdjUZIB4hzi1BttZhsNYPoAZx
"""

#Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from google.colab import files
#file = files.upload()  #upload file into google colab session
df = pd.read_csv("Liver Patient Dataset.csv",encoding = 'unicode_escape')
df.head()

#Information about the columns in the dataframe
df.info()

#Statistical details about the columns in the dataframe
df.describe()

#Converting value in Result column from 2 to 0
df['Result'] = df['Result'].replace(to_replace=2, value=0)

#Checking if there are any null values in the dataset
df.isnull()

#Checking for count of null values in each column
df.isnull().sum()

#Imputing null categorical variables with mode
for i in df.columns:
    if (df[i].dtype=='object'):
        df[i].fillna(df[i].mode()[0], inplace=True)

df.isnull().sum()

#One hot Encoding of the gender of the patient
pd.get_dummies(df['Gender of the patient'], prefix = 'Gender of the patient').head()

#Adding the new columns to original df in a new df
df1 = pd.concat([df,pd.get_dummies(df['Gender of the patient'], prefix = 'Gender of the patient')], axis=1)

#df1.head()

#Dropping existing Gender column
df1 = df1.drop(columns=['Gender of the patient'])
df1.head()

#Checking the null count of all columns
df1.isnull().sum()

#Checking information of new df
df1.info()

#Imputing numerical missing values with median
for i in df1.columns:
    if (df1[i].dtype=='float64' or df1[i].dtype=='int64'):
        df1[i].fillna(df1[i].median(), inplace=True)

#df1.head()

#Checking the null count of all columns
df1.isnull().sum()

df2 = df1.copy()

#Defining the column names
df2.columns = ['Age of the patient','Total Bilirubin','Direct Bilirubin','Alkphos Alkaline Phosphotase','Sgpt Alamine Aminotransferase','Sgot Aspartate Aminotransferase','Total Protiens','ALB Albumin','A/G Ratio Albumin and Globulin Ratio','Result','Gender of the patient_Female','Gender of the patient_Male']
df2

df2.head()

#Plotting correlation heatmap for the Liver Disease
plt.figure(figsize=(19,9))
plot=sns.heatmap(df2.corr(), vmin=-1, vmax=1, annot=True, cmap="YlOrRd")
plot.set_title("Liver Disease Correlation Heatmap")
plt.show()

correlation_matrix = df2.corr()

# Extract correlation scores
correlation_scores = correlation_matrix.unstack().sort_values()

# Filter out duplicate entries and self-correlations (diagonal)
correlation_scores = correlation_scores[correlation_scores.index.get_level_values(0) != correlation_scores.index.get_level_values(1)]

# Display correlation scores
print(correlation_scores)

high_correlations = correlation_scores[correlation_scores > 0.5]
print(high_correlations)

#Dropping highly correlated columns
df_new1 = df2.drop(columns=['ALB Albumin','Gender of the patient_Male','Direct Bilirubin'])
df_new1

# Define a custom color palette for 0 (green) and 1 (red)
custom_palette = {0: '#7DBF80', 1: '#FF9999'}

# Plotting with custom colors and adding index
sns.countplot(data=df_new1, x='Result', label='Number of patients', palette=custom_palette)

# Customize tick labels
plt.xticks(ticks=[0, 1], labels=['Healthy', 'Liver Disease'])

# Displaying the count of patients with and without liver disease
Positive, Negative = df_new1['Result'].value_counts()
print('Number of patients diagnosed with liver disease: ', Positive)
print('Number of patients not diagnosed with liver disease: ', Negative)

import seaborn as sns
import matplotlib.pyplot as plt

# Convert 'Result' to string type
df['Result'] = df['Result'].astype(str)

# Define a color palette
custom_palette = {"0": "#7DBF80", "1": "#FF9999"}

# Create a FacetGrid with histograms
g = sns.FacetGrid(df, col="Result", row="Gender of the patient", margin_titles=True, palette=custom_palette)
g.map(sns.histplot, "Age of the patient", bins=20, kde=False)

# Adjust layout and add title
plt.subplots_adjust(top=0.9)
g.fig.suptitle('Liver Disease Based on Gender and Age', fontsize=16)

# Add axis labels
g.set_axis_labels("Age of the patient", "Frequency")

# Show the plot
plt.show()

# Plotting patient Age vs Gender using categorical plots
sns.catplot(data=df, x="Age of the patient", y="Gender of the patient", hue="Result")

# Plotting Gender(Male/Female) along with Total_Bilirubin and Direct_Bilirubin using scatterplots
g = sns.FacetGrid(df, col="Gender of the patient", row="Result", margin_titles=True)
g.map(plt.scatter,"\xa0Sgpt Alamine Aminotransferase", "Sgot Aspartate Aminotransferase", edgecolor="r")
plt.subplots_adjust(top=0.9)

from scipy import stats

# Assuming df is your DataFrame
numeric_columns = df_new1.select_dtypes(include=['float64', 'int64']).columns

# Calculate Z-scores for each numeric column
z_scores = stats.zscore(df_new1[numeric_columns])

# Create a DataFrame with Z-scores
z_score_df = pd.DataFrame(z_scores, columns=numeric_columns)

# Identify outliers based on a Z-score threshold (e.g., 3)
outliers = (z_score_df > 3) | (z_score_df < -3)

# Display rows with at least one outlier
outliers_rows = df[outliers.any(axis=1)]
print(outliers_rows)

# Identify rows to keep (non-outliers)
rows_to_keep = ~outliers.any(axis=1)

# Create a DataFrame without outliers
df_new2 = df_new1[rows_to_keep]

# Display the resulting DataFrame without outliers
print(df_new2)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler

# Splitting DataFrame into train and test sets
train, test = train_test_split(df_new2, test_size=0.30, random_state=42)

# Extracting features and target variable
X_train = train.drop('Result', axis=1)
y_train = train['Result']
X_test = test.drop('Result', axis=1)
y_test = test['Result']

# Scaling features using RobustScaler
rsc = RobustScaler()
rsc.fit(X_train)
X_train = rsc.transform(X_train)
X_test = rsc.transform(X_test)

# Displaying dimensions of train and test datasets
X_train.shape, X_test.shape, y_train.shape, y_test.shape

#Plotting the Result classes using histograms
ib = px.histogram(train, y="Result")
ib.update_layout(bargap=0.2)
ib.update_layout(title = "Imbalanced Classes")
ib.show()

#Using SMOTE to balance out classes
import imblearn
from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTETomek

Sm = SMOTETomek()
X_train_new,y_train_new=Sm.fit_resample(X_train,y_train)

#Printing the count of the imbalanced and balanced classes
from collections import Counter
print("Actual Classes",Counter(y_train))
print("SMOTE Classes",Counter(y_train_new))

#Finding dimensions of test and train datas
X_train_new.shape , y_train_new.shape, X_test.shape , y_test.shape

#Plotting the Result classes using histograms
im1 = px.histogram(y=y_train_new, color_discrete_sequence=["indianred"])
im1.update_layout(bargap=0.2)
im1.update_layout(title = "Balanced Classes")
im1.show()

"""## Model Training

## 1. Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import plotly.express as px

# Train the Logistic Regression model on the SMOTE-enhanced training set
lor = LogisticRegression()
lor.fit(X_train_new, y_train_new)

# Make predictions on the training set
y_train_predict = lor.predict(X_train_new)

# Print training scores
print('Logistic Regression Training Score: {:.2%}'.format(lor.score(X_train_new, y_train_new)))

# Evaluate training set
print('\nTraining Set Metrics:')
print('Accuracy:', accuracy_score(y_train_new, y_train_predict))
print('Precision:', precision_score(y_train_new, y_train_predict))
print('Recall:', recall_score(y_train_new, y_train_predict))
print('F1 Score:', f1_score(y_train_new, y_train_predict))
print('\nClassification Report:\n', classification_report(y_train_new, y_train_predict))

# Make predictions on the test set
y_test_predict = lor.predict(X_test)

# Print testing scores
print('\nLogistic Regression Testing Score: {:.2%}'.format(lor.score(X_test, y_test)))

# Evaluate test set
print('\nTest Set Metrics:')
print('Accuracy:', accuracy_score(y_test, y_test_predict))
print('Precision:', precision_score(y_test, y_test_predict))
print('Recall:', recall_score(y_test, y_test_predict))
print('F1 Score:', f1_score(y_test, y_test_predict))
print('\nClassification Report:\n', classification_report(y_test, y_test_predict))

# Plot confusion matrix
heatmap = px.imshow(confusion_matrix(y_test, y_test_predict), aspect="auto", text_auto=True, color_continuous_scale="mint")
heatmap.update_layout(title="Confusion Matrix for Logistic Regression")
heatmap.update_xaxes(title="Predicted")
heatmap.update_yaxes(title="Actual")
heatmap.show()

#Plotting ROC Curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
logit_roc_auc = roc_auc_score(y_test, lor.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, lor.predict(X_test)[:])
plt.figure()
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1])
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

"""## 2. KNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import plotly.express as px

# Train the KNN model on the SMOTE-enhanced training set
knn = KNeighborsClassifier()
knn.fit(X_train_new, y_train_new)

# Make predictions on the training set
y_train_predict_knn = knn.predict(X_train_new)

# Print training scores
print('K-Nearest Neighbors Training Score: {:.2%}'.format(knn.score(X_train_new, y_train_new)))

# Evaluate training set
print('\nTraining Set Metrics:')
print('Accuracy:', accuracy_score(y_train_new, y_train_predict_knn))
print('Precision:', precision_score(y_train_new, y_train_predict_knn))
print('Recall:', recall_score(y_train_new, y_train_predict_knn))
print('F1 Score:', f1_score(y_train_new, y_train_predict_knn))
print('\nClassification Report:\n', classification_report(y_train_new, y_train_predict_knn))

# Make predictions on the test set
y_test_predict_knn = knn.predict(X_test)

# Print testing scores
print('\nK-Nearest Neighbors Testing Score: {:.2%}'.format(knn.score(X_test, y_test)))

# Evaluate test set
print('\nTest Set Metrics:')
print('Accuracy:', accuracy_score(y_test, y_test_predict_knn))
print('Precision:', precision_score(y_test, y_test_predict_knn))
print('Recall:', recall_score(y_test, y_test_predict_knn))
print('F1 Score:', f1_score(y_test, y_test_predict_knn))
print('\nClassification Report:\n', classification_report(y_test, y_test_predict_knn))

# Plot confusion matrix for KNN
heatmap_knn = px.imshow(confusion_matrix(y_test, y_test_predict_knn), aspect="auto", text_auto=True, color_continuous_scale="mint")
heatmap_knn.update_layout(title="Confusion Matrix for K-Nearest Neighbors")
heatmap_knn.update_xaxes(title="Predicted")
heatmap_knn.update_yaxes(title="Actual")
heatmap_knn.show()

#Plotting ROC Curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
knn_roc_auc = roc_auc_score(y_test, y_test_predict_knn)
fpr, tpr, thresholds = roc_curve(y_test, y_test_predict_knn)
plt.figure()
plt.plot(fpr, tpr, label='KNN (area = %0.2f)' % knn_roc_auc)
plt.plot([0, 1], [0, 1])
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

"""## 3. Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import plotly.express as px

# Train the Random Forest model on the SMOTE-enhanced training set
rf = RandomForestClassifier()
rf.fit(X_train_new, y_train_new)

# Make predictions on the training set
y_train_predict_rf = rf.predict(X_train_new)

# Print training scores
print('Random Forest Training Score: {:.2%}'.format(rf.score(X_train_new, y_train_new)))

# Evaluate training set
print('\nTraining Set Metrics:')
print('Accuracy:', accuracy_score(y_train_new, y_train_predict_rf))
print('Precision:', precision_score(y_train_new, y_train_predict_rf))
print('Recall:', recall_score(y_train_new, y_train_predict_rf))
print('F1 Score:', f1_score(y_train_new, y_train_predict_rf))
print('\nClassification Report:\n', classification_report(y_train_new, y_train_predict_rf))

# Make predictions on the test set
y_test_predict_rf = rf.predict(X_test)

# Print testing scores
print('\nRandom Forest Testing Score: {:.2%}'.format(rf.score(X_test, y_test)))

# Evaluate test set
print('\nTest Set Metrics:')
print('Accuracy:', accuracy_score(y_test, y_test_predict_rf))
print('Precision:', precision_score(y_test, y_test_predict_rf))
print('Recall:', recall_score(y_test, y_test_predict_rf))
print('F1 Score:', f1_score(y_test, y_test_predict_rf))
print('\nClassification Report:\n', classification_report(y_test, y_test_predict_rf))

# Plot confusion matrix
heatmap_rf = px.imshow(confusion_matrix(y_test, y_test_predict_rf), aspect="auto", text_auto=True, color_continuous_scale="mint")
heatmap_rf.update_layout(title="Confusion Matrix for Random Forest")
heatmap_rf.update_xaxes(title="Predicted")
heatmap_rf.update_yaxes(title="Actual")
heatmap_rf.show()

from sklearn.metrics import confusion_matrix

# Confusion matrix for Random Forest on the test set
conf_matrix_rf = confusion_matrix(y_test, y_test_predict_rf)

# Print the confusion matrix
print("Confusion Matrix for Random Forest:")
print(conf_matrix_rf)

# Plot ROC Curve
y_test_probs_rf = rf.predict_proba(X_test)[:, 1]
rf_roc_auc = roc_auc_score(y_test, y_test_probs_rf)
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_test_probs_rf)

plt.figure()
plt.plot(fpr_rf, tpr_rf, label='Random Forest (area = %0.2f)' % rf_roc_auc)
plt.plot([0, 1], [0, 1])
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Random Forest')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier

# Assuming X and y are your feature matrix and target variable
X = your_feature_matrix
y = your_target_variable

# Create a Random Forest Classifier (or any other model you are using)
model = RandomForestClassifier()

# Specify the number of folds for cross-validation (e.g., 5)
num_folds = 5

# Create a stratified k-fold object (stratified ensures class distribution is preserved in each fold)
cv = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)

# Perform cross-validation and get accuracy scores for each fold
cross_val_results = cross_val_score(model, X, y, cv=cv, scoring='accuracy')

# Print the accuracy scores for each fold
for fold, accuracy in enumerate(cross_val_results, 1):
    print(f'Fold {fold}: Accuracy = {accuracy:.2%}')

# Calculate and print the mean accuracy across all folds
mean_accuracy = cross_val_results.mean()
print(f'\nMean Accuracy Across Folds: {mean_accuracy:.2%}')

"""## 4. Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import plotly.express as px

# Train the Decision Tree model on the SMOTE-enhanced training set
decision_tree = DecisionTreeClassifier()
decision_tree.fit(X_train_new, y_train_new)

# Make predictions on the training set
y_train_predict_dt = decision_tree.predict(X_train_new)

# Print training scores
print('Decision Tree Training Score: {:.2%}'.format(decision_tree.score(X_train_new, y_train_new)))

# Evaluate training set
print('\nTraining Set Metrics:')
print('Accuracy:', accuracy_score(y_train_new, y_train_predict_dt))
print('Precision:', precision_score(y_train_new, y_train_predict_dt))
print('Recall:', recall_score(y_train_new, y_train_predict_dt))
print('F1 Score:', f1_score(y_train_new, y_train_predict_dt))
print('\nClassification Report:\n', classification_report(y_train_new, y_train_predict_dt))

# Make predictions on the test set
y_test_predict_dt = decision_tree.predict(X_test)

# Print testing scores
print('\nDecision Tree Testing Score: {:.2%}'.format(decision_tree.score(X_test, y_test)))

# Evaluate test set
print('\nTest Set Metrics:')
print('Accuracy:', accuracy_score(y_test, y_test_predict_dt))
print('Precision:', precision_score(y_test, y_test_predict_dt))
print('Recall:', recall_score(y_test, y_test_predict_dt))
print('F1 Score:', f1_score(y_test, y_test_predict_dt))
print('\nClassification Report:\n', classification_report(y_test, y_test_predict_dt))

# Plot confusion matrix
heatmap = px.imshow(confusion_matrix(y_test, y_test_predict_dt), aspect="auto", text_auto=True, color_continuous_scale="mint")
heatmap.update_layout(title="Confusion Matrix for Decision Tree")
heatmap.update_xaxes(title="Predicted")
heatmap.update_yaxes(title="Actual")
heatmap.show()

from sklearn.metrics import confusion_matrix

# Confusion matrix for Decision Tree on the test set
conf_matrix_decision_tree = confusion_matrix(y_test, y_test_predict_rf)

# Print the confusion matrix
print("Confusion Matrix for Decision Tree:")
print(conf_matrix_rf)

# Make predictions on the test set
y_test_probs_dt = decision_tree.predict_proba(X_test)[:, 1]

# Calculate ROC curve and AUC
fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test, y_test_probs_dt)
auc_dt = roc_auc_score(y_test, y_test_probs_dt)

# Plot ROC Curve
plt.figure()
plt.plot(fpr_dt, tpr_dt, label='Decision Tree (area = %0.2f)' % auc_dt)
plt.plot([0, 1], [0, 1])
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Decision Tree')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()